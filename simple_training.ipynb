{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62720484",
   "metadata": {},
   "source": [
    "# Data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad544c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging \n",
    "import numpy as np\n",
    "import importlib\n",
    "import pandas as pd\n",
    "from os.path import join\n",
    "from data.data_access import Data\n",
    "from preprocessing import pre\n",
    "from utils.logs import set_logging\n",
    "from config_path import ENH_LOG_PATH\n",
    "\n",
    "params_file = 'train/params/P1000/pnet/onsplit_average_reg_10_tanh_large_testing.py'\n",
    "\n",
    "log_dir = join(ENH_LOG_PATH, 'log')\n",
    "log_dir = log_dir\n",
    "set_logging(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bbfe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = importlib.machinery.SourceFileLoader('params', params_file)\n",
    "params = loader.load_module()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e651489",
   "metadata": {},
   "outputs": [],
   "source": [
    "params  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b7de32",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(**params.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8e6853",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_validate_, x_test_, y_train, y_validate_, y_test_, info_train, info_validate_, info_test_, cols = data.get_train_validate_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adf1827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "info_t = info_test_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ce7404",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"x_train:\", x_train.shape)\n",
    "print(\"x_train:\", y_train.shape)\n",
    "\n",
    "print(\"x_validate_:\", x_validate_.shape)\n",
    "print(\"y_validate_:\", y_validate_.shape)\n",
    "\n",
    "print(\"x_test:\", x_test_.shape)\n",
    "print(\"y_test:\", y_test_.shape)\n",
    "\n",
    "print(\"columns:\", len(cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b04e63",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32952750",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from model import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c0bea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params_ = deepcopy(params.models[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c884ebbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Model(**model_params_['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfa191b",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, x_validate_, y_validate_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729f0a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = model.predict(x_test_)\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "def evaluate_classification_binary(y_test, y_pred, y_pred_score=None):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    if y_pred_score is None:\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred, pos_label=1)\n",
    "    else:\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_score, pos_label=1)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    f1 = metrics.f1_score(y_test, y_pred)\n",
    "    precision = metrics.precision_score(y_test, y_pred)\n",
    "    recall = metrics.recall_score(y_test, y_pred)\n",
    "    logging.info(metrics.classification_report(y_test, y_pred))\n",
    "    from sklearn.metrics import average_precision_score\n",
    "    aupr = average_precision_score(y_test, y_pred_score)\n",
    "    score = {}\n",
    "    score['accuracy'] = accuracy\n",
    "    score['precision'] = precision\n",
    "    score['auc'] = auc\n",
    "    score['f1'] = f1\n",
    "    score['aupr'] = aupr\n",
    "    score['recall'] = recall\n",
    "\n",
    "    # plot auc curve\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "    # Save the ROC plot\n",
    "    plt.savefig('roc_curve.png')\n",
    "\n",
    "    # Show the ROC plot\n",
    "    plt.show()\n",
    "\n",
    "    return score\n",
    "\n",
    "if hasattr(model, 'predict_proba'):\n",
    "    y_pred_test_scores = model.predict_proba(x_test_)[:, 1]\n",
    "else:\n",
    "    y_pred_test_scores = y_pred_test\n",
    "\n",
    "test_score = evaluate_classification_binary(y_test_, y_pred_test, y_pred_test_scores)\n",
    "\n",
    "logging.info('Test score {}'.format(test_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03daa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python analysis/run_it_all.py\n",
    "\n",
    "model_name = 'P-net'\n",
    "\n",
    "# save prediction data for plot generation\n",
    "\n",
    "def save_prediction(info, y_pred, y_pred_scores, y_test, model_name, training=False):\n",
    "\n",
    "        if training:\n",
    "            file_name = join(model_name + '_training.csv')\n",
    "        else:\n",
    "            file_name = join(model_name + '_testing.csv')\n",
    "        info = pd.DataFrame(index=info)\n",
    "        print(('info', info))\n",
    "        print(y_test)\n",
    "        info['pred'] = y_pred\n",
    "        info['pred_scores'] = y_pred_scores\n",
    "\n",
    "        # survival case\n",
    "        # https://docs.scipy.org/doc/numpy/user/basics.rec.html\n",
    "        if y_test.dtype.fields is not None:\n",
    "            fields = y_test.dtype.fields\n",
    "            for f in fields:\n",
    "                info['y_{}'.format(f)] = y_test[f]\n",
    "        else:\n",
    "            info['y'] = y_test\n",
    "        info.to_csv(file_name)\n",
    "\n",
    "save_prediction(info_t, y_pred_test, y_pred_test_scores, y_test_, model_name)\n",
    "\n",
    "# save model\n",
    "\n",
    "filename = '/PROJECTS/Sally/PNET_py3_enh_gene/_logs/enh_vs_genes/log/fs/P-net.h5'\n",
    "model.save_model(filename)\n",
    "\n",
    "# filename = join(log_dir, 'fs')\n",
    "# filename = join(filename, model_name + '.h5')\n",
    "# # if not exists(filename.strip()):\n",
    "# #     makedirs(filename)\n",
    "\n",
    "# save model weights \n",
    "\n",
    "# w_filename = '/PROJECTS/Sally/PNET_py3_enh_gene/_logs/enh_vs_genes/log/fs/P-net_weights.h5'\n",
    "\n",
    "# model.save_model(w_filename)\n",
    "\n",
    "# load model \n",
    "\n",
    "model.load_model(filename)\n",
    "\n",
    "#load model weights\n",
    "\n",
    "# model = model.load_model(w_filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1837e6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a314587a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# shap implementation\n",
    "\n",
    "import shap   \n",
    "# from tensorflow.keras.models import load_model, model_from_json\n",
    "\n",
    "sample = x_test_[1:11]\n",
    "\n",
    "# sample_output = model.predict(sample)\n",
    "\n",
    "# # Find the indices where the value is 1\n",
    "# indices_of_ones = np.where(sample_output == 1)[0]\n",
    "\n",
    "# # Print the indices\n",
    "# print(\"Indices of '1':\", indices_of_ones)\n",
    "\n",
    "# print('sample_output:', sample_output)\n",
    "\n",
    "from config_path import INTERACTIONS_PATH\n",
    "\n",
    "gene_names = join(INTERACTIONS_PATH, 'enh_vs_genes_selected_genes.csv')\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(gene_names)\n",
    "\n",
    "# Extract gene names into a list\n",
    "genes_list = df['genes'].tolist()\n",
    "\n",
    "explainer = shap.Explainer(model, x_test_)\n",
    "shap_values = explainer.shap_values(sample)\n",
    "shap.summary_plot(shap_values, sample, feature_names=genes_list)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bbdd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap.plots.force(explainer.expected_value, shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381b6102-a60a-40f1-8431-a13e555bab47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sally_PNET_py3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
